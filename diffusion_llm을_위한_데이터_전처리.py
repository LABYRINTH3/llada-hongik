# -*- coding: utf-8 -*-
"""DIffusion LLM을 위한 데이터 전처리.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11btSkeJ1ZC14bYn2DPcuIKtinOpfDYkg

# 데이터전처리

# 1. 데이터 로드 및 확인

이미 train과 validation가 약 10 : 1 비율로 나누어져서 train 210만, valid 21만개가 저장되어 있으며, train데이터중에서 길이가 약 550 ~ 1100 사이인 데이터들만 선택했을 때 약 160만개의 이야기들이 있다. 이미 이 데이터만 해도 너무 많으므로 valid는 고의적으로 누락시키며 train 160만개중 60만개를 랜덤하게 선택한다(60만개를 50만개 train, 10만개 test 데이터로 나누고

```
# 코드로 형식 지정됨
```

 저장하고 사용할 계획이다.)
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install datasets==2.16.0

"""# [HuggingFace 데이터셋 바로가기](https://huggingface.co/datasets/roneneldan/TinyStories)"""

from datasets import load_dataset, load_from_disk,Dataset
from google.colab import drive
import os

ds = load_dataset("roneneldan/TinyStories")  # Huggingface에서 바로 가져온다.

print(ds)
print(ds['train'][0])
print(ds['validation'][0])

MIN_LEN, MAX_LEN = 550, 1100

def in_char_range(example):
    n = len(example["text"])
    return (MIN_LEN <= n <= MAX_LEN)

train_f = ds["train"].filter(in_char_range, num_proc=4)
print("filtered train:", len(train_f))

sample_600k = train_f.shuffle(seed=42).select(range(600_000))
print("sample_600k:", len(sample_600k))
split = sample_600k.train_test_split(test_size=100_000, seed=42, shuffle=False)
train_500k, test_100k = split["train"], split["test"]

"""#2. Tokenizing 적용

transformer 라이브러리 AutoTokenizer 사용 (bert-base-uncased)


---



truncation=True       : max_length를 넘는 토큰은 오른쪽을 잘라서 512 토큰으로 맞춤

padding="max_length"  : 짧은 입력은 [PADDING]으로 채워서 길이를 512로 고정 (max_length=512)

return_attention_mask : [PADDING]은 0, 실제 토큰은 1인 마스크 벡터도 같이 반환

return_token_type_ids=False : token_type_ids: 문장 쌍 태스크에서 세그먼트 구분용 ID 불필요하므로 누락

---

토크나이징 결과



1. features: ['input_ids', 'attention_mask']

2. input_ids: 토큰화된 정수 시퀀스
attention_mask: 어떤 토큰을“실제로 계산에 포함할지" 1은 유효 토큰, 0은 패딩.
"""

from transformers import AutoTokenizer
tok = AutoTokenizer.from_pretrained("bert-base-uncased")

def tok_fn(batch):
    return tok(
        batch["text"],
        truncation=True,
        padding="max_length",
        max_length=512,
        return_attention_mask=True,
        return_token_type_ids=False
    )

train_tok = train_500k.map(tok_fn, batched=True, remove_columns=["text"], num_proc=2)
test_tok  = test_100k.map(tok_fn,  batched=True, remove_columns=["text"], num_proc=2)

# 인코딩 잘되었나 확인
train_tok

ex = train_tok[0]
print(type(ex), ex.keys())
print("len(input_ids)   =", len(ex["input_ids"]))
print("len(attention_mask) =", len(ex["attention_mask"]))
print(ex["input_ids"][:20])
print(ex["attention_mask"][:320])

"""# 3. Mask data 생성

BERT식 마스킹의 80/10/10 규칙을 따른다.

1. 80%의 토큰을 MASK 토큰으로 교체한다.

2. 10%의 토큰을 임의의 토큰(단어) 으로 교체한다.

3. 10% 토큰은 어떠한 변경도 하지 않는다.

---


결과값(예시):

features: ['input_ids', 'attention_mask', 'labels', 'mask_prob']
1.  "input_ids":      [101, 2026, 2171, 2003, 103, 1012,  ... , 0, 0, 0],
2.  "attention_mask": [1,   1,    1,    1,    1,   1,     ... , 0, 0, 0],
3.  "labels":         [-100,-100,-100,-100, 2003,-100,   ... ,-100,-100,-100],
4.  "mask_prob":      0.4



"""

from transformers import DataCollatorForLanguageModeling

# 마스킹 적용 함수
def make_masked(ds_tok, tokenizer, p, batch_size=64):
    collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=True,
        mlm_probability=p,
    )
    def mask_batch(batch):

        examples = [
            {"input_ids": ids, "attention_mask": am}
            for ids, am in zip(batch["input_ids"], batch["attention_mask"])
        ]
        masked = collator(examples)
        return {
            "input_ids": masked["input_ids"],
            "attention_mask": masked["attention_mask"],
            "labels": masked["labels"],     # 마스크된 위치만 원래 토큰, 나머지는 -100
            "mask_prob": [p] * len(masked["input_ids"]),
        }

    return ds_tok.map(
        mask_batch,
        batched=True,
        batch_size=batch_size,
        remove_columns=ds_tok.column_names,
    )

train_tok_20 = make_masked(train_tok, tok, 0.2)

import numpy as np
import torch

print(train_tok_20)              # Dataset 크기/포맷
print(train_tok_20.features)     # 스키마

ex = train_tok_20[0]
print(ex.keys())                 # dict_keys(['input_ids','attention_mask','labels','mask_prob'])
print("lens:", len(ex["input_ids"]), len(ex["attention_mask"]), len(ex["labels"]))
print("mask_prob:", ex["mask_prob"])

# 실제 마스크 비율 대충 검산
emp = (np.array(ex["labels"]) != -100).mean()
print("empirical mask ratio ~", round(emp, 3))

train_tok_40 = make_masked(train_tok, tok, 0.4)

train_tok_60 = make_masked(train_tok, tok, 0.6)

train_tok_80 = make_masked(train_tok, tok, 0.8)

test_tok_60  = make_masked(test_tok,  tok, 0.6)

# 데이터값 확인
import numpy as np

# 데이터값 확인 (숫자만)
def peek(ds, name, idx=54, head=32, pair_limit=50):
    ex = ds[idx]
    print(f"\n== {name}[{idx}] ==")
    print("keys:", ex.keys())

    ids  = np.array(ex["input_ids"])
    labs = np.array(ex["labels"])
    attn = np.array(ex["attention_mask"], dtype=bool)

    # 유효 길이 / 경험적 마스크 비율
    valid_len = int(attn.sum())
    emp = float((labs[attn] != -100).mean()) if valid_len > 0 else 0.0
    print(f"유효 길이: {valid_len} / {len(ids)}")
    print("유효 마스킹 비율", round(emp, 3))


    masked_pos = (labs != -100)
    print("input_ids   =", ids[:head].tolist())
    print("labels     =", labs[:head].tolist())          # 비마스크는 -100
    print("attn      =", attn[:head].astype(int).tolist())
    print("masked_pos    =", masked_pos[:head].astype(int).tolist())

    cnt = 0
    for i in range(valid_len):


peek(train_tok_20, "train_tok_20", idx=0)
peek(train_tok_40, "train_tok_40", idx=0)
peek(train_tok_60, "train_tok_60", idx=0)
peek(train_tok_80, "train_tok_80", idx=0)
peek(test_tok_60,  "test_tok_60",  idx=0)

"""# 4. 데이터 저장"""

from google.colab import drive
drive.mount('/content/drive')

"""A. 기존 마운트 싹 해제

from google.colab import drive
drive.flush_and_unmount()

B. 폴더 흔적 제거(있으면)

import shutil, os, time
shutil.rmtree('/content/drive', ignore_errors=True)
time.sleep(1)

C. 강제 리마운트

drive.mount('/content/drive', force_remount=True)

D. 정말 MyDrive가 보이는지 확인

import os, subprocess, textwrap
print("LS /content/drive:", os.listdir('/content/drive'))
print("LS MyDrive:", os.listdir('/content/drive/MyDrive'))

드라이브에 시험 파일 쓰기

test_path = "/content/drive/MyDrive/_colab_write_test.txt"
with open(test_path, "w", encoding="utf-8") as f:
    f.write("hi drive\n")
print("WROTE:", test_path)

디스크 잔여 용량 체크 (드라이브 용량 모자르면 당연히 실패)

import shutil
usage = shutil.disk_usage("/content/drive/MyDrive")
print("Drive Free (GB):", round(usage.free / 1024**3, 2))
"""

import os, shutil, datetime

OUT_ROOT = "/content/drive/MyDrive/tinystories_export"
os.makedirs(OUT_ROOT, exist_ok=True)

def save_hf(ds, name, overwrite=False):
    path = os.path.join(OUT_ROOT, name)
    # save_to_disk는 대상 폴더가 비어있지 않으면 실패한다
    if os.path.exists(path):
        if overwrite:
            shutil.rmtree(path)
        else:
            ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            path = f"{path}_{ts}"
    ds.save_to_disk(path)
    print(f"✓ Saved {name} ({len(ds)} rows) → {path}")
    return path

import os, shutil, datetime

OUT_ROOT = "/content/drive/MyDrive/tinystories_export"
os.makedirs(OUT_ROOT, exist_ok=True)

def save_hf(ds, name, overwrite=False):
    path = os.path.join(OUT_ROOT, name)
    # save_to_disk는 대상 폴더가 비어있지 않으면 실패한다
    if os.path.exists(path):
        if overwrite:
            shutil.rmtree(path)
        else:
            ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
            path = f"{path}_{ts}"
    ds.save_to_disk(path)
    print(f"✓ Saved {name} ({len(ds)} rows) → {path}")
    return path

save_hf(train_tok_20, "train_tok_20", overwrite=False)
save_hf(train_tok_40, "train_tok_40")
save_hf(train_tok_60, "train_tok_60")
save_hf(train_tok_80, "train_tok_80")
save_hf(test_tok_60,  "test_tok_60")

"""# 되는지 확인"""

from datasets import load_from_disk
chk = load_from_disk("/content/drive/MyDrive/tinystories_export/train_tok_20")
print(chk)
print(chk[0].keys())  # ['input_ids','attention_mask','labels','mask_prob']
print(chk[0]['input_ids'])