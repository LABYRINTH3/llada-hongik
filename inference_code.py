# 7. ì¶”ë¡  : í…ìŠ¤íŠ¸ ìƒì„± - ìœ¤í¬ë¹ˆ

def mask_inputs(input_ids, t, mask_token_id, prompt_length):
    B, L = input_ids.shape
    gen_region = torch.zeros_like(input_ids, dtype=torch.bool)
    # í”„ë¡¬í”„íŠ¸ ì˜ì—­(0~Lp)ì„ ì œì™¸í•œ ìƒì„± ì˜ì—­(Lp ì´í›„)ë§Œ True
    gen_region[:, prompt_length:] = True  

    # t í™•ë¥ ì— ë”°ë¼ ë§ˆìŠ¤í‚¹í•  ìœ„ì¹˜ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ
    rand = torch.rand((B, L), device=input_ids.device)
    step_mask = rand < t.view(B, 1)
    
    # ìƒì„± ì˜ì—­(gen_region)ì´ë©´ì„œ ëœë¤ìœ¼ë¡œ ì„ íƒëœ(step_mask) ìœ„ì¹˜ë§Œ ë§ˆìŠ¤í‚¹ ìœ„ì¹˜ë¡œ í™•ì •
    mask_pos = gen_region & step_mask

    # í™•ì •ëœ ìœ„ì¹˜ë¥¼ [MASK] í† í° IDë¡œ ëŒ€ì²´
    noised = input_ids.clone()
    noised[mask_pos] = mask_token_id
    return noised, mask_pos


def sample_from_model_with_log(model, tokenizer, prompt_ids,
                               response_length=20, # ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ 20ìœ¼ë¡œ ì„¤ì • (ìš”ì²­ ë°˜ì˜)
                               steps=40, device='cuda'):
    model.eval()
    B, Lp = prompt_ids.shape
    R = response_length

    # ìƒì„±í•  í† í°(R)ë§Œí¼ [MASK] í† í°ìœ¼ë¡œ ì±„ì›Œì§„ ì´ˆê¸° ì‘ë‹µ ìƒì„±
    response = torch.full((B, R),
                          tokenizer.mask_token_id,
                          dtype=torch.long,
                          device=device)

    # í”„ë¡¬í”„íŠ¸ + [MASK] ì‘ë‹µ ê²°í•©
    combined = torch.cat([prompt_ids.to(device), response], dim=1)

    # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ (1.0ì—ì„œ 0.0ìœ¼ë¡œ ì ì§„ì  ê°ì†Œ)
    t_schedule = torch.linspace(1.0, 0.0, steps, device=device)

    print(f"\nğŸš€ í…ìŠ¤íŠ¸ ìƒì„± ì •ì œ ê³¼ì • ì‹œì‘ (Steps: {steps}, Response Length: {R})")
    print("----------------------------------------------------------------------")
    
    # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ì¶œë ¥ (ìš”ì²­ ë°˜ì˜)
    initial_prompt = tokenizer.decode(prompt_ids[0], skip_special_tokens=True)
    print(f"Initial Prompt: '{initial_prompt}'")
    print("----------------------------------------------------------------------")


    for step in range(steps):
        t = t_schedule[step].expand(B)
        
        # 1. í˜„ì¬ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ t í™•ë¥ ë§Œí¼ ëœë¤ ë§ˆìŠ¤í‚¹
        noised_inputs, mask_pos = mask_inputs(
            combined, t, tokenizer.mask_token_id, Lp
        )
        
        # 2. ë§ˆìŠ¤í¬ëœ ì…ë ¥ì— ëŒ€í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ë¡œì§“ íšë“
        logits = model(noised_inputs)
        preds = logits.argmax(-1)
        
        # 3. ì •ì œ: ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ë§Œ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        combined[mask_pos] = preds[mask_pos]
        
        # 4. ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜„ì¬ ìƒíƒœ ì¶œë ¥ (t ê°’ ì œê±° ìš”ì²­ ë°˜ì˜)
        current_text = tokenizer.decode(combined[0], skip_special_tokens=True)
        print(f"Step {step+1}/{steps}: {current_text}") 
        
    print("----------------------------------------------------------------------")
    
    # ìƒì„±ëœ í† í° ë¶€ë¶„ë§Œ ë°˜í™˜
    return combined[:, Lp:]
