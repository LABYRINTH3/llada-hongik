# 7. ì¶”ë¡  : í…ìŠ¤íŠ¸ ìƒì„± - ìœ¤í¬ë¹ˆ

def mask_inputs(input_ids, t, mask_token_id, prompt_length):
    B, L = input_ids.shape
    gen_region = torch.zeros_like(input_ids, dtype=torch.bool)
    # í”„ë¡¬í”„íŠ¸ ì˜ì—­(0~Lp)ì„ ì œì™¸í•œ ìƒì„± ì˜ì—­(Lp ì´í›„)ë§Œ True
    gen_region[:, prompt_length:] = True  

    # t í™•ë¥ ì— ë”°ë¼ ë§ˆìŠ¤í‚¹í•  ìœ„ì¹˜ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ
    rand = torch.rand((B, L), device=input_ids.device)
    step_mask = rand < t.view(B, 1)
    
    # ìƒì„± ì˜ì—­(gen_region)ì´ë©´ì„œ ëœë¤ìœ¼ë¡œ ì„ íƒëœ(step_mask) ìœ„ì¹˜ë§Œ ë§ˆìŠ¤í‚¹ ìœ„ì¹˜ë¡œ í™•ì •
    mask_pos = gen_region & step_mask

    # í™•ì •ëœ ìœ„ì¹˜ë¥¼ [MASK] í† í° IDë¡œ ëŒ€ì²´
    noised = input_ids.clone()
    noised[mask_pos] = mask_token_id
    return noised, mask_pos


def sample_from_model_with_log(model, tokenizer, prompt_ids,
                               response_length=20, # ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ 20ìœ¼ë¡œ ì„¤ì •
                               steps=40, device='cuda'):
    model.eval()
    B, Lp = prompt_ids.shape
    R = response_length

    # ìƒì„±í•  í† í°(R)ë§Œí¼ [MASK] í† í°ìœ¼ë¡œ ì±„ì›Œì§„ ì´ˆê¸° ì‘ë‹µ ìƒì„±
    response = torch.full((B, R),
                          tokenizer.mask_token_id,
                          dtype=torch.long,
                          device=device)

    # í”„ë¡¬í”„íŠ¸ + [MASK] ì‘ë‹µ ê²°í•©
    combined = torch.cat([prompt_ids.to(device), response], dim=1)

    # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ (1.0ì—ì„œ 0.0ìœ¼ë¡œ ì ì§„ì  ê°ì†Œ)
    t_schedule = torch.linspace(1.0, 0.0, steps, device=device)

    print(f"\nğŸš€ í…ìŠ¤íŠ¸ ìƒì„± ì •ì œ ê³¼ì • ì‹œì‘ (Steps: {steps}, Response Length: {R})")
    print("----------------------------------------------------------------------")
    
    # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ì¶œë ¥ 
    initial_prompt = tokenizer.decode(prompt_ids[0], skip_special_tokens=True)
    print(f"Initial Prompt: '{initial_prompt}'")
    print("----------------------------------------------------------------------")


    for step in range(steps):
        t = t_schedule[step].expand(B)
        
        # 1. í˜„ì¬ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ t í™•ë¥ ë§Œí¼ ëœë¤ ë§ˆìŠ¤í‚¹
        noised_inputs, mask_pos = mask_inputs(
            combined, t, tokenizer.mask_token_id, Lp
        )
        
        # 2. ë§ˆìŠ¤í¬ëœ ì…ë ¥ì— ëŒ€í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ë¡œì§“ íšë“
        logits = model(noised_inputs)
        preds = logits.argmax(-1)
        
        # 3. ì •ì œ: ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ë§Œ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        combined[mask_pos] = preds[mask_pos]
        
        # 4. ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜„ì¬ ìƒíƒœ ì¶œë ¥ (t ê°’ ì œê±° ìš”ì²­ ë°˜ì˜)
        current_text = tokenizer.decode(combined[0], skip_special_tokens=True)
        print(f"Step {step+1}/{steps}: {current_text}") 
        
    print("----------------------------------------------------------------------")
    
    # ìƒì„±ëœ í† í° ë¶€ë¶„ë§Œ ë°˜í™˜
    return combined[:, Lp:]


# 8. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ - ìœ¤í¬ë¹ˆ
import glob
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import get_linear_schedule_with_warmup 
import torch.optim as optim
import os # os ë¼ì´ë¸ŒëŸ¬ë¦¬ import í™•ì¸ (ë§Œì•½ ìµœìƒë‹¨ì— ì—†ë‹¤ë©´ ì¶”ê°€ í•„ìš”)


checkpoint_dir = "./weight" 
if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("Diffusion ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘ (tinystories_masked_p_mask80.pt ë¡œê·¸ ì¶œë ¥)")
    print("=" * 70)

    # tinystories_masked_p_mask80.pt ì²´í¬í¬ì¸íŠ¸ë§Œ ì‚¬ìš©
    # ì°¸ê³ : í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ê²½ë¡œì™€ íŒŒì¼ëª…ì„ ì •í™•í•˜ê²Œ í™•ì¸.
    ckpt_path = os.path.join(checkpoint_dir, "tinystories_masked_p_mask80.pt")
    
    if not os.path.exists(ckpt_path):
        raise FileNotFoundError(f"ìš”ì²­ëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ckpt_path}")
    
    # í”„ë¡¬í”„íŠ¸ ì„¤ì •
    prompt_text = "hello. world!"        # <-  ì¶”ë¡ ì˜ input.. ë°”ê¿”ë„ ë˜ì§€ë§Œ ì¼ë‹¨ì€ ì´ê±¸ë¡œ ê³ ì •
    prompt_ids = tokenizer.encode(prompt_text, return_tensors="pt")

    print("\nğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘:", os.path.basename(ckpt_path))

    # 1) ëª¨ë¸ ì„ ì–¸
    model = MaskedDiffusionTransformer(tokenizer.vocab_size).to(device)

    # 2) state_dict ë¡œë“œ (compile ì œê±° ì²˜ë¦¬ í¬í•¨)
    raw_state = torch.load(ckpt_path, map_location=device)
    new_state = {}
    for k, v in raw_state.items():
        # torch.compile ì‚¬ìš© í”ì (_orig_mod.) ì œê±°
        new_key = k[len("_orig_mod."):] if k.startswith("_orig_mod.") else k
        new_state[new_key] = v
    model.load_state_dict(new_state)

    print("   â†’ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # 3) inference ì‹¤í–‰ (ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œ)
    out_ids = sample_from_model_with_log(
        model,
        tokenizer,
        prompt_ids=prompt_ids,
        steps=40,
        device=device
    )

    generated = tokenizer.decode(out_ids[0], skip_special_tokens=True)
    print("\n" + "=" * 70)
    print("âœ¨ ìµœì¢… ìƒì„± ê²°ê³¼:")
    print("     ", generated)
    print("=" * 70)