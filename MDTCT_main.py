# ì „ì²˜ë¦¬ - í† í¬ë‚˜ì´ì§• (Tokenizing) - ê¹€ê¸°í˜„

# 0. ê¸°ë³¸ ì„¤ì •
# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
epochs_per_stage = 2
batch_size = 256
lr = 1e-4

# 1. í† í¬ë‚˜ì´ì € (Tokenizer) ì„¤ì • - ê¹€ê¸°í˜„



# 2.1 ëª¨ë¸ ì •ì˜: Masked Diffusion Transformer - ì •ì—°ìš±
import torch
import torch.nn as nn
import torch.nn.functional as F
from datasets import load_from_disk
import os

# ----------------------------------------
# ğŸ”¹ 1. Transformer ì¸ì½”ë” ë¸”ë¡ ì§ì ‘ êµ¬í˜„
# ----------------------------------------
class TransformerEncoderBlock(nn.Module):
    def __init__(self, hidden_dim=512, num_heads=8, ffn_dim=2048, dropout=0.1):
        super().__init__()
        # (1) Multi-Head Self-Attention
        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.dropout1 = nn.Dropout(dropout)

        # (2) Feed Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, ffn_dim),
            nn.ReLU(),
            nn.Linear(ffn_dim, hidden_dim)
        )
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, attention_mask=None):
        # attention_mask: 1ì€ ìœ ì§€, 0ì€ íŒ¨ë”© (BERTì™€ ë™ì¼í•œ ê·œì¹™)
        key_padding_mask = None
        if attention_mask is not None:
            key_padding_mask = ~attention_mask.bool()  # (1->False, 0->True)

        # (1) Self-Attention + ì”ì°¨ ì—°ê²° + ì •ê·œí™”
        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)
        x = self.norm1(x + self.dropout1(attn_out))

        # (2) FFN + ì”ì°¨ ì—°ê²° + ì •ê·œí™”
        ffn_out = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_out))
        return x


# ----------------------------------------
# ğŸ”¹ 2. ì „ì²´ ëª¨ë¸ êµ¬ì¡° ì •ì˜
# ----------------------------------------
class MaskedDiffusionTransformer(nn.Module):
    def __init__(self, vocab_size=30522, hidden_dim=512, num_layers=6, num_heads=8, ffn_dim=2048, max_length=512):
        super().__init__()

        # (1) ì„ë² ë”©: ë‹¨ì–´ + ìœ„ì¹˜ ì •ë³´ ê²°í•©
        self.token_emb = nn.Embedding(vocab_size, hidden_dim)
        self.pos_emb = nn.Embedding(max_length, hidden_dim)

        # (2) ì§ì ‘ êµ¬ì„±í•œ Transformer ì¸ì½”ë” ë ˆì´ì–´ 6ê°œ
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderBlock(hidden_dim, num_heads, ffn_dim)
            for _ in range(num_layers)
        ])
        self.norm = nn.LayerNorm(hidden_dim)

        # (3) ì¶œë ¥ì¸µ: ê° í† í°ë³„ë¡œ ë‹¨ì–´ ì˜ˆì¸¡ (vocab í¬ê¸°ë§Œí¼ í™•ë¥ )
        self.output_layer = nn.Linear(hidden_dim, vocab_size)

    def forward(self, input_ids, attention_mask=None):
        B, L = input_ids.shape
        pos = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)

        # (1) ì…ë ¥ ì„ë² ë”©
        x = self.token_emb(input_ids) + self.pos_emb(pos)

        # (2) ì¸ì½”ë” ë¸”ë¡ í†µê³¼
        for layer in self.encoder_layers:
            x = layer(x, attention_mask)

        x = self.norm(x)

        # (3) ì¶œë ¥ì¸µ
        logits = self.output_layer(x)  # [batch, seq_len, vocab_size]
        return logits

# ì¥ì¹˜ ì„¤ì • (GPU or CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = MaskedDiffusionTransformer().to(device)
print(" ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (device =", device, ")")

# 2.2ì†ì‹¤ í•¨ìˆ˜ (Loss Function) ì •ì˜ - ì •ì—°ìš±
import torch.nn.functional as F

def diffusion_loss(logits, labels, mask_pos=None):
    """
    logits: [batch, seq_len, vocab_size]
    labels: [batch, seq_len]
    mask_pos: 
    -100ì¸ í† í°ì€ ë¬´ì‹œí•˜ê³  ì†ì‹¤ ê³„ì‚°
    """
    vocab_size = logits.size(-1)
    loss = F.cross_entropy(
        logits.view(-1, vocab_size),
        labels.view(-1),
        ignore_index=-100
    )
    return loss

# 3. ë°ì´í„°ì…‹ ë¡œë”© í•¨ìˆ˜ - ì •ì—°ìš±
BASE_DIR = "data/tinystories_export"
device = "cuda" if torch.cuda.is_available() else "cpu"
# 3. ë°ì´í„°ì…‹ ë¡œë”© í•¨ìˆ˜ - ì •ì—°ìš±
from datasets import load_from_disk
import os

def load_dataset_by_mask_prob(mask_prob):
    print(f"===== ë§ˆìŠ¤í¬ {mask_prob}% ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘ =====")
    # 1. ì¸í’‹(mask_prob)ì„ ì‚¬ìš©í•´ ê²½ë¡œ ìƒì„±
    path = f"{BASE_DIR}/train_tok_{mask_prob}"
    # 2. í•´ë‹¹ ë°ì´í„°ì…‹ ë¡œë“œ
    if not os.path.exists(path):
        print(f"ê²½ê³ : {path}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {path}")
    ds = load_from_disk(path)
    print(f"  -> {path} ë¡œë“œ ì™„ë£Œ (ìƒ˜í”Œ ìˆ˜: {len(ds)})")
    # 3. PyTorch í˜•ì‹ ì„¤ì •
    ds.set_format(
        type="torch", 
        columns=["input_ids", "labels", "attention_mask"]
    )
    print("  -> PyTorch í…ì„œ í˜•ì‹ìœ¼ë¡œ ì„¤ì • ì™„ë£Œ")
    return ds

# 4. í•™ìŠµ í•¨ìˆ˜ (Training) - ì´íƒœí›ˆ
def train_stage(model, dataloader, optimizer, scheduler):
    # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì • - Dropout í™œì„±í™”
    model.train()
    total_loss = 0.0

    for batch in dataloader:

        input_ids = batch["input_ids"].to(device) # input_ids - ë§ˆìŠ¤í¬ëœ í† í°ì´ í¬í•¨ëœ ë¬¸ì¥
        labels = batch["labels"].to(device) # labels - ë§ˆìŠ¤í¬ ìœ„ì¹˜ì˜ ì •ë‹µ í† í° (ì •ë‹µì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ -100)

        # ë§ˆìŠ¤í¬ ìœ„ì¹˜ ì •ë³´ ë§Œë“¤ê¸°
            # ì›ë˜ í† í¬ë‚˜ì´ì§• í•˜ë©´ì„œ ë§ˆìŠ¤í¬ ìœ„ì¹˜ ì •ë³´ë„ ë„˜ê²¨ì¤„ê¹Œ ê³ ë ¤í–ˆì§€ë§Œ ë°ì´í„°ì…‹ í¬ê¸°ë¥¼ ê³ ë ¤í•´ ë°°ì¹˜ ë‚´ì—ì„œ ìƒì„±
        mask_pos = labels.ne(-100)
        
        # Forward Propagation ë° Loss ê³„ì‚°
        logits = model(input_ids) # logits: [ë°°ì¹˜í¬ê¸° (ë¬¸ì¥ ê°œìˆ˜), ì‹œí€€ìŠ¤ê¸¸ì´(í•œë¬¸ì¥ì´ í† í°ìˆ˜ ê³ ì •), ë‹¨ì–´ì‚¬ì „í¬ê¸°(bert ì‚¬ì „í¬ê¸°)]ì˜ ì ìˆ˜
        loss = diffusion_loss(logits, labels, mask_pos) # ì—°ìš±ë‹˜ì´ ë§Œë“  í•¨ìˆ˜
        
        # Backpropagation
        optimizer.zero_grad() # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
        loss.backward() # ì—­ì „íŒŒ
        optimizer.step() # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸

        scheduler.step() # í•™ìŠµ ì§„í–‰í•  ìˆ˜ë¡ lr ê°ì†Œì‹œí‚´
        
        total_loss += loss.item()
    return total_loss / len(dataloader)


# 5. ê²€ì¦ í•¨ìˆ˜ (Validation) - ì´íƒœí›ˆ
def evaluate(model, dataloader, device='cuda'):
    model.eval()
    # í‰ê°€ ëª¨ë“œ - Dropout ë¹„í™œì„±í™”
    total_loss = 0.0
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    with torch.no_grad():

        # ìœ„ ì™€ ë™ì¼
        for batch in dataloader:

            input_ids = batch["input_ids"].to(device) 
            labels = batch["labels"].to(device)
            mask_pos = labels.ne(-100)
            
            logits = model(input_ids)
            loss = diffusion_loss(logits, labels, mask_pos)
            
            total_loss += loss.item()
    return total_loss / len(dataloader)


# 6. ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ - ì´íƒœí›ˆ
def curriculum_train():
    # ëª¨ë¸ ìƒì„± í›„ GPUë¡œ ì´ë™
    model = MaskedDiffusionTransformer(tokenizer.vocab_size).to(device)
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    print(f" ëª¨ë¸ ìƒì„± íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}ê°œ")
    
    model = torch.compile(model)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
    
    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    prev_ckpt = None
    
    dataset_name = "tinystories_masked_p"
    probs = tinystory_probs # í™•ë¥  ë¦¬ìŠ¤íŠ¸ [0.2, 0.4, 0.6, 0.8]

    # ê° ë§ˆìŠ¤í‚¹ í™•ë¥ ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ
    for p in probs:
        # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
        ds_all = load_dataset(dataset_name, p)
        
        # ë°ì´í„°ì…‹ì„ í•™ìŠµ/ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„í• 
        split = ds_all.train_test_split(test_size=0.1, seed=77)
        ds_train = split["train"] # í•™ìŠµ ë°ì´í„°ì…‹ (90%)
        ds_validation = split["test"] # ê²€ì¦ ë°ì´í„°ì…‹ (10%)

        print(f" í•™ìŠµ ë°ì´í„°: {len(ds_train)}ê°œ")
        print(f" ê²€ì¦ ë°ì´í„°: {len(ds_validation)}ê°œ")

        # í•™ìŠµìš© DataLoader
        train_loader = DataLoader(
            ds_train,
            batch_size=batch_size,
            shuffle=True,
            num_workers=4, # ì½”ë© ê²°ì œí•˜ë©´ cpu ì½”ì–´ìˆ˜ 4ê°œ...
            prefetch_factor=4 # ê° ì›Œì»¤ê°€ ë¯¸ë¦¬ ë¡œë“œí•˜ëŠ” ë°°ì¹˜ ìˆ˜
        )
        
        # ê²€ì¦ìš© DataLoader
        val_loader = DataLoader(
            ds_validation,
            batch_size=batch_size,
            shuffle=False,
            num_workers=4,
            prefetch_factor=4
        )
        
        # ì´ì „ ë‹¨ê³„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        if prev_ckpt:
            model.load_state_dict(torch.load(prev_ckpt, map_location=device))
            print(f" ì´ì „ ë‹¨ê³„ ê°€ì¤‘ì¹˜ ë¡œë“œ: {prev_ckpt}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        total_steps = epochs_per_stage * len(train_loader)

        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        # í—ˆê¹…í˜ì´ìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜´ - ëª¨ë¸ í•™ìŠµë¥  ë™ì ìœ¼ë¡œ ì¡°ì ˆ
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=total_steps // 10,
            num_training_steps=total_steps
        )
        
        # í•™ìŠµ ë£¨í”„
        for epoch in range(1, epochs_per_stage + 1):
            print(f"\n Epoch {epoch}/{epochs_per_stage}")
            train_loss = train_stage(model, train_loader, optimizer, scheduler) # í•™ìŠµ - ì´íƒœí›ˆ
            val_loss = evaluate(model, val_loader, device) # ê²€ì¦ í•¨ìˆ˜ - ì´íƒœí›ˆ

            print(f" Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        ckpt_path = os.path.join(checkpoint_dir, f"tinystories_mask{int(p*100)}.pt")
        torch.save(model.state_dict(), ckpt_path)
        print(f"\n ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {ckpt_path}")

        # ì´ì „ ë‹¨ê³„ ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
        prev_ckpt = ckpt_path
    print(f" í•™ìŠµ ì™„ë£Œ!!!!!!!!!!")


# 7. ì¶”ë¡  : í…ìŠ¤íŠ¸ ìƒì„± - ìœ¤í¬ë¹ˆ 
from transformers import AutoTokenizer
import torch

def sample_from_model(model, tokenizer, prompt_text="Once upon a time", steps=10, max_length=30):
    """
    Diffusion ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì ì§„ì ìœ¼ë¡œ ë³µì›í•˜ëŠ” í•¨ìˆ˜
    í•™ìŠµì´ ì•ˆëœ ëª¨ë¸ì¼ ê²½ìš°, ì¶œë ¥ì€ ëœë¤í•˜ì§€ë§Œ êµ¬ì¡° í…ŒìŠ¤íŠ¸ ìš©ë„ë¡œ ì‚¬ìš© ê°€ëŠ¥
    """
    model.eval()
    input_ids = tokenizer.encode(prompt_text, return_tensors="pt").to(device)
    print(f"\n[ì‹œì‘ í”„ë¡¬í”„íŠ¸]: {prompt_text}")
    print("-" * 60)

    with torch.no_grad():
        for step in range(steps):
            logits = model(input_ids)                          # ëª¨ë¸ ì¶œë ¥ (ì˜ˆì¸¡ ì ìˆ˜)
            preds = torch.argmax(logits, dim=-1)               # ê° í† í°ë³„ ê°€ì¥ ë†’ì€ í™•ë¥  ì„ íƒ
            text = tokenizer.decode(preds[0], skip_special_tokens=True)  # ìˆ«ìâ†’ë‹¨ì–´ë¡œ ë³€í™˜
            print(f"Step {step+1:02d}/{steps}: {text}")

    return text



# 8. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ - ìœ¤í¬ë¹ˆ
if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("Diffusion ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 70)

    # âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

    # âœ… ëª¨ë¸ ì´ˆê¸°í™” (ëœë¤ ìƒíƒœ)
    model = MaskedDiffusionTransformer().to(device)

    generated_text = sample_from_model(
        model=model,
        tokenizer=tokenizer,
        prompt_text="Once upon a time",  # ì‹œì‘ ë¬¸ì¥
        steps=5,                         # Diffusion ìŠ¤í… ìˆ˜ (ë§ì„ìˆ˜ë¡ ì ì§„ì  ë³µì›)
        max_length=40                    # ì¶œë ¥ í† í° ìµœëŒ€ ê¸¸ì´
    )

    print("\n[ìµœì¢… ìƒì„± ê²°ê³¼]")
    print("=" * 70)
    print(generated_text)
    print("=" * 70)


'''
# ë°ì´í„° ë¡œë“œ í™•ì¸ (í…ŒìŠ¤íŠ¸ìš©)
from datasets import load_from_disk
import os

base_dir = os.path.join(os.getcwd(), "tinystories_export")
test_path = os.path.join(base_dir, "test_tok_60")

print(f"ë°ì´í„° ê²½ë¡œ í™•ì¸: {test_path}")
ds = load_from_disk(test_path)
print(f"ìƒ˜í”Œ ê°œìˆ˜: {len(ds)}")
print("features:", ds.column_names)
print("ì²« ìƒ˜í”Œ ì˜ˆì‹œ:")
print({k: ds[0][k][:10] if isinstance(ds[0][k], list) else ds[0][k] for k in ds.column_names})
'''
