from transformers import AutoTokenizer

# ì „ì²˜ë¦¬ - í† í¬ë‚˜ì´ì§• (Tokenizing) - ê¹€ê¸°í˜„

# 0. ê¸°ë³¸ ì„¤ì •
# í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°
epochs_per_stage = 2
batch_size = 256
lr = 1e-4

tinystory_probs = [0.2, 0.4, 0.6, 0.8]
# 1. í† í¬ë‚˜ì´ì € (Tokenizer) ì„¤ì • - ì´íƒœí›ˆ
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
mask_token_id = tokenizer.mask_token_id


# 2.1 ëª¨ë¸ ì •ì˜: Masked Diffusion Transformer - ì •ì—°ìš±
import torch
import torch.nn as nn
import torch.nn.functional as F
from datasets import load_from_disk
import os

# ----------------------------------------
#  1. Transformer ì¸ì½”ë” ë¸”ë¡ ì§ì ‘ êµ¬í˜„
# ----------------------------------------
class TransformerEncoderBlock(nn.Module):
    def __init__(self, hidden_dim=512, num_heads=8, ffn_dim=2048, dropout=0.1):
        super().__init__()
        # (1) Multi-Head Self-Attention
        self.self_attn = nn.MultiheadAttention(hidden_dim, num_heads, batch_first=True)
        self.norm1 = nn.LayerNorm(hidden_dim)
        self.dropout1 = nn.Dropout(dropout)

        # (2) Feed Forward Network
        self.ffn = nn.Sequential(
            nn.Linear(hidden_dim, ffn_dim),
            nn.ReLU(),
            nn.Linear(ffn_dim, hidden_dim)
        )
        self.norm2 = nn.LayerNorm(hidden_dim)
        self.dropout2 = nn.Dropout(dropout)

    def forward(self, x, attention_mask=None):
        # attention_mask: 1ì€ ìœ ì§€, 0ì€ íŒ¨ë”© (BERTì™€ ë™ì¼í•œ ê·œì¹™)
        key_padding_mask = None
        if attention_mask is not None:
            key_padding_mask = ~attention_mask.bool()  # (1->False, 0->True)

        # (1) Self-Attention + ì”ì°¨ ì—°ê²° + ì •ê·œí™”
        attn_out, _ = self.self_attn(x, x, x, key_padding_mask=key_padding_mask)
        x = self.norm1(x + self.dropout1(attn_out))

        # (2) FFN + ì”ì°¨ ì—°ê²° + ì •ê·œí™”
        ffn_out = self.ffn(x)
        x = self.norm2(x + self.dropout2(ffn_out))
        return x

'''
# ----------------------------------------
#  2. ì „ì²´ ëª¨ë¸ êµ¬ì¡° ì •ì˜
# ----------------------------------------
class MaskedDiffusionTransformer(nn.Module):
    def __init__(self, vocab_size=30522, hidden_dim=512, num_layers=6, num_heads=8, ffn_dim=2048, max_length=512):
        super().__init__()

        # (1) ì„ë² ë”©: ë‹¨ì–´ + ìœ„ì¹˜ ì •ë³´ ê²°í•©
        self.token_emb = nn.Embedding(vocab_size, hidden_dim)
        self.pos_emb = nn.Embedding(max_length, hidden_dim)

        # (2) ì§ì ‘ êµ¬ì„±í•œ Transformer ì¸ì½”ë” ë ˆì´ì–´ 6ê°œ
        self.encoder_layers = nn.ModuleList([
            TransformerEncoderBlock(hidden_dim, num_heads, ffn_dim)
            for _ in range(num_layers)
        ])
        self.norm = nn.LayerNorm(hidden_dim)

        # (3) ì¶œë ¥ì¸µ: ê° í† í°ë³„ë¡œ ë‹¨ì–´ ì˜ˆì¸¡ (vocab í¬ê¸°ë§Œí¼ í™•ë¥ )
        self.output_layer = nn.Linear(hidden_dim, vocab_size)

    def forward(self, input_ids, attention_mask=None):
        B, L = input_ids.shape
        pos = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)

        # (1) ì…ë ¥ ì„ë² ë”©
        x = self.token_emb(input_ids) + self.pos_emb(pos)

        # (2) ì¸ì½”ë” ë¸”ë¡ í†µê³¼
        for layer in self.encoder_layers:
            x = layer(x, attention_mask)

        x = self.norm(x)

        # (3) ì¶œë ¥ì¸µ
        logits = self.output_layer(x)  # [batch, seq_len, vocab_size]
        return logits

# ì¥ì¹˜ ì„¤ì • (GPU or CPU)
device = "cuda" if torch.cuda.is_available() else "cpu"
model = MaskedDiffusionTransformer().to(device)
print(" ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (device =", device, ")")

# 2.2ì†ì‹¤ í•¨ìˆ˜ (Loss Function) ì •ì˜ - ì •ì—°ìš±
import torch.nn.functional as F

def diffusion_loss(logits, labels, mask_pos=None):
    """
    logits: [batch, seq_len, vocab_size]
    labels: [batch, seq_len]
    mask_pos: 
    -100ì¸ í† í°ì€ ë¬´ì‹œí•˜ê³  ì†ì‹¤ ê³„ì‚°
    """
    vocab_size = logits.size(-1)
    loss = F.cross_entropy(
        logits.view(-1, vocab_size),
        labels.view(-1),
        ignore_index=-100
    )
    return loss
'''
class MaskedDiffusionTransformer(nn.Module):
    def __init__(self, vocab_size, d_model=512, n_layers=6, n_heads=8, max_length=512):
        super().__init__()
        self.token_emb   = nn.Embedding(vocab_size, d_model)
        self.pos_emb     = nn.Embedding(max_length, d_model)
        enc_layer       = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, batch_first=True)
        self.encoder     = nn.TransformerEncoder(enc_layer, num_layers=n_layers)
        self.output_proj = nn.Linear(d_model, vocab_size)

    def forward(self, input_ids, mask_positions=None):
        B, L = input_ids.shape
        pos = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, -1)
        x = self.token_emb(input_ids) + self.pos_emb(pos)
        x = self.encoder(x)
        return self.output_proj(x)

criterion = nn.CrossEntropyLoss(reduction='none')
def diffusion_loss(logits, target_ids, mask_positions):
    B, L, V = logits.shape
    loss_all = criterion(
        logits.view(B*L, V),
        target_ids.view(B*L)
    ).view(B, L)
    return (loss_all * mask_positions.float()).sum() \
           / mask_positions.sum().clamp_min(1.0)


# 3. ë°ì´í„°ì…‹ ë¡œë”© í•¨ìˆ˜ - ì •ì—°ìš±
BASE_DIR = "data/tinystories_export"
device = "cuda" if torch.cuda.is_available() else "cpu"
# 3. ë°ì´í„°ì…‹ ë¡œë”© í•¨ìˆ˜ - ì •ì—°ìš±
from datasets import load_from_disk
import os

def load_dataset_by_mask_prob(mask_prob):
    print(f"===== ë§ˆìŠ¤í¬ {mask_prob}% ë°ì´í„°ì…‹ ë¡œë”© ì‹œì‘ =====")
    # 1. ì¸í’‹(mask_prob)ì„ ì‚¬ìš©í•´ ê²½ë¡œ ìƒì„±
    path = f"{BASE_DIR}/train_tok_{mask_prob}"
    # 2. í•´ë‹¹ ë°ì´í„°ì…‹ ë¡œë“œ
    if not os.path.exists(path):
        print(f"ê²½ê³ : {path}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {path}")
    ds = load_from_disk(path)
    print(f"  -> {path} ë¡œë“œ ì™„ë£Œ (ìƒ˜í”Œ ìˆ˜: {len(ds)})")
    # 3. PyTorch í˜•ì‹ ì„¤ì •
    ds.set_format(
        type="torch", 
        columns=["input_ids", "labels", "attention_mask"]
    )
    print("  -> PyTorch í…ì„œ í˜•ì‹ìœ¼ë¡œ ì„¤ì • ì™„ë£Œ")
    return ds

# 4. í•™ìŠµ í•¨ìˆ˜ (Training) - ì´íƒœí›ˆ
def train_stage(model, dataloader, optimizer, scheduler):
    # ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì • - Dropout í™œì„±í™”
    model.train()
    total_loss = 0.0

    for batch in dataloader:

        input_ids = batch["input_ids"].to(device) # input_ids - ë§ˆìŠ¤í¬ëœ í† í°ì´ í¬í•¨ëœ ë¬¸ì¥
        labels = batch["labels"].to(device) # labels - ë§ˆìŠ¤í¬ ìœ„ì¹˜ì˜ ì •ë‹µ í† í° (ì •ë‹µì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ -100)

        # ë§ˆìŠ¤í¬ ìœ„ì¹˜ ì •ë³´ ë§Œë“¤ê¸°
            # ì›ë˜ í† í¬ë‚˜ì´ì§• í•˜ë©´ì„œ ë§ˆìŠ¤í¬ ìœ„ì¹˜ ì •ë³´ë„ ë„˜ê²¨ì¤„ê¹Œ ê³ ë ¤í–ˆì§€ë§Œ ë°ì´í„°ì…‹ í¬ê¸°ë¥¼ ê³ ë ¤í•´ ë°°ì¹˜ ë‚´ì—ì„œ ìƒì„±
        mask_pos = labels.ne(-100)
        
        # Forward Propagation ë° Loss ê³„ì‚°
        
        # logits: [ë°°ì¹˜í¬ê¸° (ë¬¸ì¥ ê°œìˆ˜), ì‹œí€€ìŠ¤ê¸¸ì´(í•œë¬¸ì¥ì´ í† í°ìˆ˜ ê³ ì •), ë‹¨ì–´ì‚¬ì „í¬ê¸°(bert ì‚¬ì „í¬ê¸°)]ì˜ ì ìˆ˜
        # ì¦‰, logits[i][j][k]ëŠ” ië²ˆì§¸ ë¬¸ì¥ì˜ jë²ˆì§¸ í† í°ì´ kë²ˆì§¸ ë‹¨ì–´ì¼ ì ìˆ˜
        logits = model(input_ids)
        loss = diffusion_loss(logits, labels, mask_pos) # ì—°ìš±ë‹˜ì´ ë§Œë“  í•¨ìˆ˜
        
        # Backpropagation
        optimizer.zero_grad() # ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
        loss.backward() # ì—­ì „íŒŒ
        optimizer.step() # ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸

        scheduler.step() # í•™ìŠµ ì§„í–‰í•  ìˆ˜ë¡ lr ê°ì†Œì‹œí‚´
        
        total_loss += loss.item()
    return total_loss / len(dataloader)


# 5. ê²€ì¦ í•¨ìˆ˜ (Validation) - ì´íƒœí›ˆ
def evaluate(model, dataloader, device='cuda'):
    model.eval()
    # í‰ê°€ ëª¨ë“œ - Dropout ë¹„í™œì„±í™”
    total_loss = 0.0
    
    # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° ë¹„í™œì„±í™”
    with torch.no_grad():

        # ìœ„ ì™€ ë™ì¼
        for batch in dataloader:

            input_ids = batch["input_ids"].to(device) 
            labels = batch["labels"].to(device)
            mask_pos = labels.ne(-100)
            
            logits = model(input_ids)
            loss = diffusion_loss(logits, labels, mask_pos)
            
            total_loss += loss.item()
    return total_loss / len(dataloader)


# 6. ì»¤ë¦¬í˜ëŸ¼ í•™ìŠµ - ì´íƒœí›ˆ
def curriculum_train():
    # ëª¨ë¸ ìƒì„± í›„ GPUë¡œ ì´ë™
    model = MaskedDiffusionTransformer(tokenizer.vocab_size).to(device)
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ ì¶œë ¥
    total_params = sum(p.numel() for p in model.parameters())
    print(f" ëª¨ë¸ ìƒì„± íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}ê°œ")
    
    model = torch.compile(model)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # Adam ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
    
    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ
    prev_ckpt = None
    
    dataset_name = "tinystories_masked_p"
    probs = tinystory_probs # í™•ë¥  ë¦¬ìŠ¤íŠ¸ [0.2, 0.4, 0.6, 0.8]

    # ê° ë§ˆìŠ¤í‚¹ í™•ë¥ ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµ
    for p in probs:
        # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
        ds_all = load_dataset(dataset_name, p)
        
        # ë°ì´í„°ì…‹ì„ í•™ìŠµ/ê²€ì¦ ì„¸íŠ¸ë¡œ ë¶„í• 
        split = ds_all.train_test_split(test_size=0.1, seed=77)
        ds_train = split["train"] # í•™ìŠµ ë°ì´í„°ì…‹ (90%)
        ds_validation = split["test"] # ê²€ì¦ ë°ì´í„°ì…‹ (10%)

        print(f" í•™ìŠµ ë°ì´í„°: {len(ds_train)}ê°œ")
        print(f" ê²€ì¦ ë°ì´í„°: {len(ds_validation)}ê°œ")

        # í•™ìŠµìš© DataLoader
        train_loader = DataLoader(
            ds_train,
            batch_size=batch_size,
            shuffle=True,
            num_workers=4, # ì½”ë© ê²°ì œí•˜ë©´ cpu ì½”ì–´ìˆ˜ 4ê°œ...
            prefetch_factor=4 # ê° ì›Œì»¤ê°€ ë¯¸ë¦¬ ë¡œë“œí•˜ëŠ” ë°°ì¹˜ ìˆ˜
        )
        
        # ê²€ì¦ìš© DataLoader
        val_loader = DataLoader(
            ds_validation,
            batch_size=batch_size,
            shuffle=False,
            num_workers=4,
            prefetch_factor=4
        )
        
        # ì´ì „ ë‹¨ê³„ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        if prev_ckpt:
            model.load_state_dict(torch.load(prev_ckpt, map_location=device))
            print(f" ì´ì „ ë‹¨ê³„ ê°€ì¤‘ì¹˜ ë¡œë“œ: {prev_ckpt}")
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        total_steps = epochs_per_stage * len(train_loader)

        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        # í—ˆê¹…í˜ì´ìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜´ - ëª¨ë¸ í•™ìŠµë¥  ë™ì ìœ¼ë¡œ ì¡°ì ˆ
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=total_steps // 10,
            num_training_steps=total_steps
        )
        
        # í•™ìŠµ ë£¨í”„
        for epoch in range(1, epochs_per_stage + 1):
            print(f"\n Epoch {epoch}/{epochs_per_stage}")
            train_loss = train_stage(model, train_loader, optimizer, scheduler) # í•™ìŠµ - ì´íƒœí›ˆ
            val_loss = evaluate(model, val_loader, device) # ê²€ì¦ í•¨ìˆ˜ - ì´íƒœí›ˆ

            print(f" Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}")

        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        ckpt_path = os.path.join(checkpoint_dir, f"tinystories_mask{int(p*100)}.pt")
        torch.save(model.state_dict(), ckpt_path)
        print(f"\n ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {ckpt_path}")

        # ì´ì „ ë‹¨ê³„ ì²´í¬í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
        prev_ckpt = ckpt_path
    print(f" í•™ìŠµ ì™„ë£Œ!!!!!!!!!!")


# # 7. ì¶”ë¡  : í…ìŠ¤íŠ¸ ìƒì„± - ìœ¤í¬ë¹ˆ

# def mask_inputs(input_ids, t, mask_token_id, prompt_length):
#     B, L = input_ids.shape
#     gen_region = torch.zeros_like(input_ids, dtype=torch.bool)
#     gen_region[:, prompt_length:] = True  

# #mask_input() ì•ˆì—ì„œ ë§¤ ìŠ¤í…ë§ˆë‹¤ ëœë¤ ë§ˆìŠ¤í‚¹ ë°œìƒ -> ëª¨ë¸ì´ ë‹¤ì‹œ ì±„ì›Œë„£ëŠ” ë‹¨ì–´ë„ ë‹¬ë¼ì§.
#     rand = torch.rand((B, L), device=input_ids.device)
#     step_mask = rand < t.view(B, 1)
#     mask_pos = gen_region & step_mask

#     noised = input_ids.clone()
#     noised[mask_pos] = mask_token_id
#     return noised, mask_pos


# def sample_from_model(model, tokenizer, prompt_ids,
#                       response_length=50, steps=10, device='cuda'):
#     model.eval()
#     B, Lp = prompt_ids.shape
#     R = response_length

#     response = torch.full((B, R),
#                           tokenizer.mask_token_id,
#                           dtype=torch.long,
#                           device=device)

#     combined = torch.cat([prompt_ids.to(device), response], dim=1)

#     t_schedule = torch.linspace(1.0, 0.0, steps, device=device)

#     for step in range(steps):
#         t = t_schedule[step].expand(B)
#         noised_inputs, mask_pos = mask_inputs(
#             combined, t, tokenizer.mask_token_id, Lp
#         )
#         logits = model(noised_inputs)
#         preds = logits.argmax(-1)
#         combined[mask_pos] = preds[mask_pos]

#     # ìƒì„±ëœ í† í° ë¶€ë¶„ë§Œ ë°˜í™˜
#     return combined[:, Lp:]


# # 8. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ - ìœ¤í¬ë¹ˆ
# import glob

# checkpoint_dir = "./weight" # <- weightì˜ ì €ì¥ ìœ„ì¹˜
# if __name__ == "__main__":
#     print("\n" + "=" * 70)
#     print("Diffusion ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘ (ì²´í¬í¬ì¸íŠ¸ë³„ ë¹„êµ)")
#     print("=" * 70)

#     ckpt_paths = sorted(glob.glob(os.path.join(checkpoint_dir, "*.pt")))  #weight ë¶ˆëŸ¬ì˜¤ê¸°
#     if not ckpt_paths:
#         raise FileNotFoundError("ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

#     prompt_text = "Once upon a time"        # <-  ì¶”ë¡ ì˜ input 
#     prompt_ids = tokenizer.encode(prompt_text, return_tensors="pt")

#     for ckpt_path in ckpt_paths:
#         print("\nğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘:", os.path.basename(ckpt_path))

#         # 1) ëª¨ë¸ ì„ ì–¸
#         model = MaskedDiffusionTransformer(tokenizer.vocab_size).to(device)

#         # 2) state_dict ë¡œë“œ (compile ì œê±° ì²˜ë¦¬ í¬í•¨)
#         raw_state = torch.load(ckpt_path, map_location=device)
#         new_state = {}
#         for k, v in raw_state.items():
#             new_key = k[len("_orig_mod."):] if k.startswith("_orig_mod.") else k
#             new_state[new_key] = v
#         model.load_state_dict(new_state)

#         print("   â†’ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

#         # 3) inference ì‹¤í–‰
#         out_ids = sample_from_model(
#             model,
#             tokenizer,
#             prompt_ids=prompt_ids,
#             response_length=50, #ì¶œë ¥í•  í† í°(ë‹¨ì–´) ìˆ˜
#             steps=40,         #ë°˜ë³µ íšŸìˆ˜
#             device=device
#         )

#         generated = tokenizer.decode(out_ids[0], skip_special_tokens=True)
#         print("   â†’ ìƒì„± ê²°ê³¼:")
#         print("     ", generated)
#         print("-" * 70)


# --------------------------------------------------------------------------
# ê¸°ì¡´ ì½”ë“œëŠ” ì—¬ëŸ¬ ì²´í¬í¬ì¸íŠ¸ì˜ ìµœì¢… ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ë°©ì‹ì´ì—ˆìœ¼ë‚˜,
# ë§ˆìŠ¤í‚¹ ë””í“¨ì „ ëª¨ë¸ì˜ ì‘ë™ ê³¼ì •ì„ ë³´ì—¬ì£¼ê¸° ìœ„í•´ ë‹¨ì¼ ì²´í¬í¬ì¸íŠ¸ì˜ 40ë‹¨ê³„ ì •ì œ ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ë„ë¡ ìˆ˜ì •í•¨.
# --------------------------------------------------------------------------


# 7. ì¶”ë¡  : í…ìŠ¤íŠ¸ ìƒì„± - ìœ¤í¬ë¹ˆ

def mask_inputs(input_ids, t, mask_token_id, prompt_length):
    B, L = input_ids.shape
    gen_region = torch.zeros_like(input_ids, dtype=torch.bool)
    # í”„ë¡¬í”„íŠ¸ ì˜ì—­(0~Lp)ì„ ì œì™¸í•œ ìƒì„± ì˜ì—­(Lp ì´í›„)ë§Œ True
    gen_region[:, prompt_length:] = True  

    # t í™•ë¥ ì— ë”°ë¼ ë§ˆìŠ¤í‚¹í•  ìœ„ì¹˜ë¥¼ ëœë¤ìœ¼ë¡œ ì„ íƒ
    rand = torch.rand((B, L), device=input_ids.device)
    step_mask = rand < t.view(B, 1)
    
    # ìƒì„± ì˜ì—­(gen_region)ì´ë©´ì„œ ëœë¤ìœ¼ë¡œ ì„ íƒëœ(step_mask) ìœ„ì¹˜ë§Œ ë§ˆìŠ¤í‚¹ ìœ„ì¹˜ë¡œ í™•ì •
    mask_pos = gen_region & step_mask

    # í™•ì •ëœ ìœ„ì¹˜ë¥¼ [MASK] í† í° IDë¡œ ëŒ€ì²´
    noised = input_ids.clone()
    noised[mask_pos] = mask_token_id
    return noised, mask_pos


def sample_from_model_with_log(model, tokenizer, prompt_ids,
                               response_length=20, # ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´ 20ìœ¼ë¡œ ì„¤ì •
                               steps=40, device='cuda'):
    model.eval()
    B, Lp = prompt_ids.shape
    R = response_length

    # ìƒì„±í•  í† í°(R)ë§Œí¼ [MASK] í† í°ìœ¼ë¡œ ì±„ì›Œì§„ ì´ˆê¸° ì‘ë‹µ ìƒì„±
    response = torch.full((B, R),
                          tokenizer.mask_token_id,
                          dtype=torch.long,
                          device=device)

    # í”„ë¡¬í”„íŠ¸ + [MASK] ì‘ë‹µ ê²°í•©
    combined = torch.cat([prompt_ids.to(device), response], dim=1)

    # ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ (1.0ì—ì„œ 0.0ìœ¼ë¡œ ì ì§„ì  ê°ì†Œ)
    t_schedule = torch.linspace(1.0, 0.0, steps, device=device)

    print(f"\nğŸš€ í…ìŠ¤íŠ¸ ìƒì„± ì •ì œ ê³¼ì • ì‹œì‘ (Steps: {steps}, Response Length: {R})")
    print("----------------------------------------------------------------------")
    
    # ì´ˆê¸° í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ì¶œë ¥ 
    initial_prompt = tokenizer.decode(prompt_ids[0], skip_special_tokens=True)
    print(f"Initial Prompt: '{initial_prompt}'")
    print("----------------------------------------------------------------------")


    for step in range(steps):
        t = t_schedule[step].expand(B)
        
        # 1. í˜„ì¬ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ t í™•ë¥ ë§Œí¼ ëœë¤ ë§ˆìŠ¤í‚¹
        noised_inputs, mask_pos = mask_inputs(
            combined, t, tokenizer.mask_token_id, Lp
        )
        
        # 2. ë§ˆìŠ¤í¬ëœ ì…ë ¥ì— ëŒ€í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ë¡œì§“ íšë“
        logits = model(noised_inputs)
        preds = logits.argmax(-1)
        
        # 3. ì •ì œ: ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ë§Œ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        combined[mask_pos] = preds[mask_pos]
        
        # 4. ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜„ì¬ ìƒíƒœ ì¶œë ¥ (t ê°’ ì œê±° ìš”ì²­ ë°˜ì˜)
        current_text = tokenizer.decode(combined[0], skip_special_tokens=True)
        print(f"Step {step+1}/{steps}: {current_text}") 
        
    print("----------------------------------------------------------------------")
    
    # ìƒì„±ëœ í† í° ë¶€ë¶„ë§Œ ë°˜í™˜
    return combined[:, Lp:]


# 8. ë©”ì¸ ì‹¤í–‰ ì½”ë“œ - ìœ¤í¬ë¹ˆ
import glob
from torch.utils.data import DataLoader
from datasets import load_dataset
from transformers import get_linear_schedule_with_warmup 
import torch.optim as optim
import os # os ë¼ì´ë¸ŒëŸ¬ë¦¬ import í™•ì¸ (ë§Œì•½ ìµœìƒë‹¨ì— ì—†ë‹¤ë©´ ì¶”ê°€ í•„ìš”)


checkpoint_dir = "./weight" 
if __name__ == "__main__":
    print("\n" + "=" * 70)
    print("Diffusion ê¸°ë°˜ í…ìŠ¤íŠ¸ ìƒì„± ì‹œì‘ (tinystories_masked_p_mask80.pt ë¡œê·¸ ì¶œë ¥)")
    print("=" * 70)

    # tinystories_masked_p_mask80.pt ì²´í¬í¬ì¸íŠ¸ë§Œ ì‚¬ìš©
    # ì°¸ê³ : í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ê²½ë¡œì™€ íŒŒì¼ëª…ì„ ì •í™•í•˜ê²Œ í™•ì¸.
    ckpt_path = os.path.join(checkpoint_dir, "tinystories_masked_p_mask80.pt")
    
    if not os.path.exists(ckpt_path):
        raise FileNotFoundError(f"ìš”ì²­ëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {ckpt_path}")
    
    # í”„ë¡¬í”„íŠ¸ ì„¤ì •
    prompt_text = "hello. world!"        # <-  ì¶”ë¡ ì˜ input.. ë°”ê¿”ë„ ë˜ì§€ë§Œ ì¼ë‹¨ì€ ì´ê±¸ë¡œ ê³ ì •
    prompt_ids = tokenizer.encode(prompt_text, return_tensors="pt")

    print("\nğŸ“Œ ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘:", os.path.basename(ckpt_path))

    # 1) ëª¨ë¸ ì„ ì–¸
    model = MaskedDiffusionTransformer(tokenizer.vocab_size).to(device)

    # 2) state_dict ë¡œë“œ (compile ì œê±° ì²˜ë¦¬ í¬í•¨)
    raw_state = torch.load(ckpt_path, map_location=device)
    new_state = {}
    for k, v in raw_state.items():
        # torch.compile ì‚¬ìš© í”ì (_orig_mod.) ì œê±°
        new_key = k[len("_orig_mod."):] if k.startswith("_orig_mod.") else k
        new_state[new_key] = v
    model.load_state_dict(new_state)

    print("   â†’ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # 3) inference ì‹¤í–‰ (ë¡œê·¸ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œ)
    out_ids = sample_from_model_with_log(
        model,
        tokenizer,
        prompt_ids=prompt_ids,
        steps=40,
        device=device
    )

    generated = tokenizer.decode(out_ids[0], skip_special_tokens=True)
    print("\n" + "=" * 70)
    print("âœ¨ ìµœì¢… ìƒì„± ê²°ê³¼:")
    print("     ", generated)
    print("=" * 70)